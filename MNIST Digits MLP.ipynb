{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da7c96b-6fbc-4661-af3b-b12cba879207",
   "metadata": {},
   "source": [
    "# MNIST Digits MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55749d7-f395-417f-a7ac-ae1b5f21d43c",
   "metadata": {},
   "source": [
    "A revisit of the MNIST Digits Recognition problem with MLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54762c8-3139-4ff0-94df-f9302ccd53b6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf425584-bd28-48b6-b786-fb33a30d084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mltoolkit.utils import dump_keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aead224-d755-4814-b27e-d0660480d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50067a6-4dbb-4d12-ae04-7073ccae0457",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19567a39-d431-4468-acb8-14008c593045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_array(dic:dict,path:str=\"Datasets\\\\Raw Data\\\\\",fmt:str='csv',**kwargs):\n",
    "    '''\n",
    "    Dump multiple Series/DataFrames/2d-arrays as CSV/Parquet/Pickle format \n",
    "    at once.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dic: dictionary\n",
    "        Dictionary of the 2d-arrays/Series/DataFrame to be dumped.\n",
    "        Key = filename, Value = 2d-arrays/Series/DataFrame\n",
    "    path: str\n",
    "        Path to dump the file(s)\n",
    "    fmt: str {'csv','parquet','pkl'}\n",
    "        File format to be stored.\n",
    "    **kwargs\n",
    "    '''\n",
    "    assert fmt in ('csv','parquet','pkl')\n",
    "    \n",
    "    for filename,file in dic.items():\n",
    "        if type(file) is np.ndarray or type(file) is pd.Series:\n",
    "            file = pd.DataFrame(file)\n",
    "        if type(file) is pd.DataFrame:\n",
    "            if fmt == 'csv': file.to_csv(path+filename+\".csv\",**kwargs)\n",
    "            elif fmt == 'parquet': file.to_parquet(path+filename+\".parquet\",**kwargs)\n",
    "            elif fmt == 'pkl': file.to_pickle(path+filename+\".pkl\",**kwargs)\n",
    "        else:\n",
    "            raise TypeError(\"Only Series, DataFrame and nd-array are acceptable as the \" +\n",
    "                            \"content of the passed in dictionary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2d052d-13b0-4399-84ac-78f45c23ba8d",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2cc9dce-3ca9-4879-8497-eac95c8f57b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000,), (10000, 784), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw = joblib.load(\"Datasets\\\\X_train_raw.pkl\")\n",
    "y_train_raw = joblib.load(\"Datasets\\\\y_train_raw.pkl\")\n",
    "X_test_raw = joblib.load(\"Datasets\\\\X_test_raw.pkl\")\n",
    "y_test_raw = joblib.load(\"Datasets\\\\y_test_raw.pkl\")\n",
    "X_train_raw.shape,y_train_raw.shape,X_test_raw.shape,y_test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc5215b-4646-4871-b90a-b71927260317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (55000,), (5000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_raw,y_train_raw,test_size=5000,stratify=y_train_raw)\n",
    "X_train.shape,X_valid.shape,y_train.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654ee070-6d8d-4296-b8da-576e88abfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,y_test = X_test_raw.copy(),y_test_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ead31a-3089-4bea-9fd3-f40b127c68c5",
   "metadata": {},
   "source": [
    "## Scaling the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2001dae-4984-4b97-a845-eb2d062a7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_valid /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12445a9a-5642-4447-977a-a0a589602c2d",
   "metadata": {},
   "source": [
    "### Saving Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0afe69e6-7ffc-43b0-8f70-63eeb137f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = {\n",
    "    \"X_train\":X_train,\n",
    "    \"X_valid\":X_valid,\n",
    "    \"y_train\":y_train,\n",
    "    \"y_valid\":y_valid,\n",
    "    \"X_test\":X_test,\n",
    "    \"y_test\":y_test,\n",
    "}\n",
    "dump_array(csvs,\"Datasets\\\\\",'parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc89b3-040d-422c-a06b-e8a05ebb5f99",
   "metadata": {},
   "source": [
    "## Training MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c85d9-4aaa-4717-93ae-e4c9c7feaad9",
   "metadata": {},
   "source": [
    "The initial model architecture before hyperparameters tuning is as shown in the image below:\n",
    "\n",
    "<img src=\"Images/MNIST_Digits_MLP_Architecture_0.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d0e7f7-04b7-4bcf-91f3-7adf6d5a9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_mlp = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(784,),name='flatten'),\n",
    "    keras.layers.Dense(784,activation='relu',name='hidden_1'),\n",
    "    keras.layers.Dense(784,activation='relu',name='hidden_2'),\n",
    "    keras.layers.Dense(10,activation='softmax',name='output'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "750e255c-ac68-4bff-9f7a-96dbf2d2ca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " hidden_1 (Dense)            (None, 784)               615440    \n",
      "                                                                 \n",
      " hidden_2 (Dense)            (None, 784)               615440    \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                7850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,238,730\n",
      "Trainable params: 1,238,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnist_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8355fa5-5796-4bfd-9ddf-9284beb8587b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "215/215 [==============================] - 3s 7ms/step - loss: 2.3315 - accuracy: 0.1069 - val_loss: 2.3187 - val_accuracy: 0.1192\n",
      "Epoch 2/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.3091 - accuracy: 0.1278 - val_loss: 2.2969 - val_accuracy: 0.1404\n",
      "Epoch 3/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.2876 - accuracy: 0.1501 - val_loss: 2.2758 - val_accuracy: 0.1574\n",
      "Epoch 4/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.2666 - accuracy: 0.1721 - val_loss: 2.2554 - val_accuracy: 0.1856\n",
      "Epoch 5/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.2463 - accuracy: 0.1962 - val_loss: 2.2354 - val_accuracy: 0.2054\n",
      "Epoch 6/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.2264 - accuracy: 0.2185 - val_loss: 2.2159 - val_accuracy: 0.2290\n",
      "Epoch 7/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.2069 - accuracy: 0.2444 - val_loss: 2.1967 - val_accuracy: 0.2586\n",
      "Epoch 8/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.1878 - accuracy: 0.2737 - val_loss: 2.1779 - val_accuracy: 0.2898\n",
      "Epoch 9/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.1690 - accuracy: 0.3066 - val_loss: 2.1593 - val_accuracy: 0.3238\n",
      "Epoch 10/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.1504 - accuracy: 0.3435 - val_loss: 2.1409 - val_accuracy: 0.3594\n",
      "Epoch 11/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.1319 - accuracy: 0.3791 - val_loss: 2.1227 - val_accuracy: 0.3982\n",
      "Epoch 12/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.1137 - accuracy: 0.4155 - val_loss: 2.1047 - val_accuracy: 0.4344\n",
      "Epoch 13/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.0956 - accuracy: 0.4479 - val_loss: 2.0868 - val_accuracy: 0.4670\n",
      "Epoch 14/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.0777 - accuracy: 0.4782 - val_loss: 2.0690 - val_accuracy: 0.4948\n",
      "Epoch 15/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.0598 - accuracy: 0.5063 - val_loss: 2.0514 - val_accuracy: 0.5216\n",
      "Epoch 16/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.0420 - accuracy: 0.5312 - val_loss: 2.0338 - val_accuracy: 0.5500\n",
      "Epoch 17/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.0243 - accuracy: 0.5529 - val_loss: 2.0162 - val_accuracy: 0.5732\n",
      "Epoch 18/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.0067 - accuracy: 0.5736 - val_loss: 1.9987 - val_accuracy: 0.5922\n",
      "Epoch 19/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.9891 - accuracy: 0.5914 - val_loss: 1.9813 - val_accuracy: 0.6074\n",
      "Epoch 20/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.9716 - accuracy: 0.6072 - val_loss: 1.9639 - val_accuracy: 0.6230\n",
      "Epoch 21/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.9541 - accuracy: 0.6208 - val_loss: 1.9466 - val_accuracy: 0.6342\n",
      "Epoch 22/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.9366 - accuracy: 0.6339 - val_loss: 1.9292 - val_accuracy: 0.6466\n",
      "Epoch 23/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.9191 - accuracy: 0.6449 - val_loss: 1.9119 - val_accuracy: 0.6544\n",
      "Epoch 24/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.9017 - accuracy: 0.6547 - val_loss: 1.8947 - val_accuracy: 0.6636\n",
      "Epoch 25/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.8843 - accuracy: 0.6646 - val_loss: 1.8774 - val_accuracy: 0.6694\n",
      "Epoch 26/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.8669 - accuracy: 0.6721 - val_loss: 1.8602 - val_accuracy: 0.6776\n",
      "Epoch 27/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.8496 - accuracy: 0.6798 - val_loss: 1.8430 - val_accuracy: 0.6842\n",
      "Epoch 28/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.8323 - accuracy: 0.6866 - val_loss: 1.8258 - val_accuracy: 0.6894\n",
      "Epoch 29/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.8149 - accuracy: 0.6932 - val_loss: 1.8087 - val_accuracy: 0.6940\n",
      "Epoch 30/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.7977 - accuracy: 0.6985 - val_loss: 1.7915 - val_accuracy: 0.6994\n",
      "Epoch 31/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.7804 - accuracy: 0.7043 - val_loss: 1.7744 - val_accuracy: 0.7030\n",
      "Epoch 32/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.7632 - accuracy: 0.7097 - val_loss: 1.7574 - val_accuracy: 0.7062\n",
      "Epoch 33/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.7460 - accuracy: 0.7148 - val_loss: 1.7404 - val_accuracy: 0.7120\n",
      "Epoch 34/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.7289 - accuracy: 0.7192 - val_loss: 1.7234 - val_accuracy: 0.7160\n",
      "Epoch 35/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.7118 - accuracy: 0.7232 - val_loss: 1.7065 - val_accuracy: 0.7184\n",
      "Epoch 36/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.6947 - accuracy: 0.7268 - val_loss: 1.6896 - val_accuracy: 0.7216\n",
      "Epoch 37/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.6777 - accuracy: 0.7300 - val_loss: 1.6728 - val_accuracy: 0.7252\n",
      "Epoch 38/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.6608 - accuracy: 0.7331 - val_loss: 1.6561 - val_accuracy: 0.7274\n",
      "Epoch 39/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.6439 - accuracy: 0.7365 - val_loss: 1.6394 - val_accuracy: 0.7320\n",
      "Epoch 40/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.6271 - accuracy: 0.7395 - val_loss: 1.6228 - val_accuracy: 0.7350\n",
      "Epoch 41/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.6104 - accuracy: 0.7427 - val_loss: 1.6063 - val_accuracy: 0.7374\n",
      "Epoch 42/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.5937 - accuracy: 0.7460 - val_loss: 1.5898 - val_accuracy: 0.7406\n",
      "Epoch 43/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.5772 - accuracy: 0.7487 - val_loss: 1.5735 - val_accuracy: 0.7430\n",
      "Epoch 44/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.5607 - accuracy: 0.7510 - val_loss: 1.5573 - val_accuracy: 0.7450\n",
      "Epoch 45/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.5444 - accuracy: 0.7533 - val_loss: 1.5411 - val_accuracy: 0.7480\n",
      "Epoch 46/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.5281 - accuracy: 0.7558 - val_loss: 1.5251 - val_accuracy: 0.7496\n",
      "Epoch 47/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.5120 - accuracy: 0.7585 - val_loss: 1.5092 - val_accuracy: 0.7528\n",
      "Epoch 48/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.4960 - accuracy: 0.7607 - val_loss: 1.4934 - val_accuracy: 0.7562\n",
      "Epoch 49/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.4801 - accuracy: 0.7630 - val_loss: 1.4777 - val_accuracy: 0.7590\n",
      "Epoch 50/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.4644 - accuracy: 0.7650 - val_loss: 1.4622 - val_accuracy: 0.7620\n",
      "Epoch 51/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.4487 - accuracy: 0.7676 - val_loss: 1.4468 - val_accuracy: 0.7654\n",
      "Epoch 52/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.4333 - accuracy: 0.7692 - val_loss: 1.4316 - val_accuracy: 0.7686\n",
      "Epoch 53/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.4179 - accuracy: 0.7714 - val_loss: 1.4165 - val_accuracy: 0.7698\n",
      "Epoch 54/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.4028 - accuracy: 0.7738 - val_loss: 1.4016 - val_accuracy: 0.7714\n",
      "Epoch 55/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.3877 - accuracy: 0.7757 - val_loss: 1.3868 - val_accuracy: 0.7728\n",
      "Epoch 56/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.3729 - accuracy: 0.7779 - val_loss: 1.3722 - val_accuracy: 0.7744\n",
      "Epoch 57/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.3582 - accuracy: 0.7799 - val_loss: 1.3577 - val_accuracy: 0.7762\n",
      "Epoch 58/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.3437 - accuracy: 0.7822 - val_loss: 1.3434 - val_accuracy: 0.7786\n",
      "Epoch 59/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.3293 - accuracy: 0.7844 - val_loss: 1.3293 - val_accuracy: 0.7808\n",
      "Epoch 60/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.3151 - accuracy: 0.7863 - val_loss: 1.3154 - val_accuracy: 0.7822\n",
      "Epoch 61/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.3011 - accuracy: 0.7881 - val_loss: 1.3016 - val_accuracy: 0.7838\n",
      "Epoch 62/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.2873 - accuracy: 0.7896 - val_loss: 1.2880 - val_accuracy: 0.7874\n",
      "Epoch 63/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.2737 - accuracy: 0.7911 - val_loss: 1.2746 - val_accuracy: 0.7902\n",
      "Epoch 64/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.2602 - accuracy: 0.7925 - val_loss: 1.2614 - val_accuracy: 0.7922\n",
      "Epoch 65/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.2470 - accuracy: 0.7944 - val_loss: 1.2484 - val_accuracy: 0.7936\n",
      "Epoch 66/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.2339 - accuracy: 0.7957 - val_loss: 1.2356 - val_accuracy: 0.7948\n",
      "Epoch 67/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.2210 - accuracy: 0.7973 - val_loss: 1.2229 - val_accuracy: 0.7956\n",
      "Epoch 68/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.2083 - accuracy: 0.7986 - val_loss: 1.2105 - val_accuracy: 0.7974\n",
      "Epoch 69/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.1958 - accuracy: 0.8001 - val_loss: 1.1982 - val_accuracy: 0.7988\n",
      "Epoch 70/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.1835 - accuracy: 0.8011 - val_loss: 1.1861 - val_accuracy: 0.7998\n",
      "Epoch 71/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.1714 - accuracy: 0.8024 - val_loss: 1.1743 - val_accuracy: 0.8012\n",
      "Epoch 72/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.1595 - accuracy: 0.8038 - val_loss: 1.1626 - val_accuracy: 0.8024\n",
      "Epoch 73/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.1478 - accuracy: 0.8048 - val_loss: 1.1511 - val_accuracy: 0.8038\n",
      "Epoch 74/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.1362 - accuracy: 0.8061 - val_loss: 1.1397 - val_accuracy: 0.8050\n",
      "Epoch 75/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.1249 - accuracy: 0.8075 - val_loss: 1.1286 - val_accuracy: 0.8062\n",
      "Epoch 76/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.1137 - accuracy: 0.8087 - val_loss: 1.1177 - val_accuracy: 0.8074\n",
      "Epoch 77/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.1027 - accuracy: 0.8097 - val_loss: 1.1069 - val_accuracy: 0.8084\n",
      "Epoch 78/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.0919 - accuracy: 0.8109 - val_loss: 1.0963 - val_accuracy: 0.8094\n",
      "Epoch 79/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.0813 - accuracy: 0.8120 - val_loss: 1.0859 - val_accuracy: 0.8116\n",
      "Epoch 80/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.0709 - accuracy: 0.8129 - val_loss: 1.0757 - val_accuracy: 0.8130\n",
      "Epoch 81/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.0607 - accuracy: 0.8144 - val_loss: 1.0657 - val_accuracy: 0.8140\n",
      "Epoch 82/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.0506 - accuracy: 0.8153 - val_loss: 1.0558 - val_accuracy: 0.8150\n",
      "Epoch 83/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.0407 - accuracy: 0.8164 - val_loss: 1.0462 - val_accuracy: 0.8156\n",
      "Epoch 84/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.0310 - accuracy: 0.8172 - val_loss: 1.0367 - val_accuracy: 0.8166\n",
      "Epoch 85/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.0215 - accuracy: 0.8183 - val_loss: 1.0273 - val_accuracy: 0.8182\n",
      "Epoch 86/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.0122 - accuracy: 0.8194 - val_loss: 1.0181 - val_accuracy: 0.8198\n",
      "Epoch 87/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.0030 - accuracy: 0.8203 - val_loss: 1.0091 - val_accuracy: 0.8206\n",
      "Epoch 88/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9939 - accuracy: 0.8211 - val_loss: 1.0003 - val_accuracy: 0.8208\n",
      "Epoch 89/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9851 - accuracy: 0.8221 - val_loss: 0.9916 - val_accuracy: 0.8214\n",
      "Epoch 90/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9764 - accuracy: 0.8230 - val_loss: 0.9831 - val_accuracy: 0.8224\n",
      "Epoch 91/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9678 - accuracy: 0.8239 - val_loss: 0.9747 - val_accuracy: 0.8228\n",
      "Epoch 92/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9595 - accuracy: 0.8251 - val_loss: 0.9665 - val_accuracy: 0.8234\n",
      "Epoch 93/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9512 - accuracy: 0.8260 - val_loss: 0.9584 - val_accuracy: 0.8252\n",
      "Epoch 94/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9431 - accuracy: 0.8268 - val_loss: 0.9505 - val_accuracy: 0.8254\n",
      "Epoch 95/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9352 - accuracy: 0.8278 - val_loss: 0.9427 - val_accuracy: 0.8260\n",
      "Epoch 96/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9274 - accuracy: 0.8287 - val_loss: 0.9351 - val_accuracy: 0.8274\n",
      "Epoch 97/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9197 - accuracy: 0.8294 - val_loss: 0.9276 - val_accuracy: 0.8278\n",
      "Epoch 98/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9122 - accuracy: 0.8301 - val_loss: 0.9202 - val_accuracy: 0.8288\n",
      "Epoch 99/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.9048 - accuracy: 0.8308 - val_loss: 0.9130 - val_accuracy: 0.8296\n",
      "Epoch 100/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8976 - accuracy: 0.8316 - val_loss: 0.9059 - val_accuracy: 0.8298\n",
      "Epoch 101/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8905 - accuracy: 0.8323 - val_loss: 0.8989 - val_accuracy: 0.8302\n",
      "Epoch 102/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8835 - accuracy: 0.8330 - val_loss: 0.8921 - val_accuracy: 0.8304\n",
      "Epoch 103/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8766 - accuracy: 0.8336 - val_loss: 0.8853 - val_accuracy: 0.8312\n",
      "Epoch 104/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8699 - accuracy: 0.8343 - val_loss: 0.8787 - val_accuracy: 0.8320\n",
      "Epoch 105/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8633 - accuracy: 0.8349 - val_loss: 0.8723 - val_accuracy: 0.8322\n",
      "Epoch 106/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8568 - accuracy: 0.8353 - val_loss: 0.8659 - val_accuracy: 0.8332\n",
      "Epoch 107/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8504 - accuracy: 0.8358 - val_loss: 0.8597 - val_accuracy: 0.8342\n",
      "Epoch 108/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8442 - accuracy: 0.8365 - val_loss: 0.8535 - val_accuracy: 0.8350\n",
      "Epoch 109/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8380 - accuracy: 0.8372 - val_loss: 0.8475 - val_accuracy: 0.8358\n",
      "Epoch 110/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8320 - accuracy: 0.8378 - val_loss: 0.8416 - val_accuracy: 0.8360\n",
      "Epoch 111/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8260 - accuracy: 0.8385 - val_loss: 0.8358 - val_accuracy: 0.8368\n",
      "Epoch 112/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8202 - accuracy: 0.8389 - val_loss: 0.8301 - val_accuracy: 0.8372\n",
      "Epoch 113/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8145 - accuracy: 0.8395 - val_loss: 0.8244 - val_accuracy: 0.8382\n",
      "Epoch 114/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8088 - accuracy: 0.8400 - val_loss: 0.8189 - val_accuracy: 0.8392\n",
      "Epoch 115/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8033 - accuracy: 0.8403 - val_loss: 0.8135 - val_accuracy: 0.8398\n",
      "Epoch 116/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7979 - accuracy: 0.8411 - val_loss: 0.8082 - val_accuracy: 0.8404\n",
      "Epoch 117/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7925 - accuracy: 0.8418 - val_loss: 0.8030 - val_accuracy: 0.8414\n",
      "Epoch 118/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7873 - accuracy: 0.8423 - val_loss: 0.7978 - val_accuracy: 0.8416\n",
      "Epoch 119/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7821 - accuracy: 0.8432 - val_loss: 0.7928 - val_accuracy: 0.8418\n",
      "Epoch 120/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7770 - accuracy: 0.8438 - val_loss: 0.7878 - val_accuracy: 0.8422\n",
      "Epoch 121/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7721 - accuracy: 0.8440 - val_loss: 0.7829 - val_accuracy: 0.8422\n",
      "Epoch 122/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7672 - accuracy: 0.8444 - val_loss: 0.7781 - val_accuracy: 0.8426\n",
      "Epoch 123/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7623 - accuracy: 0.8452 - val_loss: 0.7734 - val_accuracy: 0.8428\n",
      "Epoch 124/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7576 - accuracy: 0.8455 - val_loss: 0.7688 - val_accuracy: 0.8434\n",
      "Epoch 125/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7530 - accuracy: 0.8459 - val_loss: 0.7642 - val_accuracy: 0.8434\n",
      "Epoch 126/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7484 - accuracy: 0.8466 - val_loss: 0.7598 - val_accuracy: 0.8440\n",
      "Epoch 127/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7439 - accuracy: 0.8470 - val_loss: 0.7554 - val_accuracy: 0.8442\n",
      "Epoch 128/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7394 - accuracy: 0.8473 - val_loss: 0.7510 - val_accuracy: 0.8446\n",
      "Epoch 129/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7351 - accuracy: 0.8477 - val_loss: 0.7468 - val_accuracy: 0.8448\n",
      "Epoch 130/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7308 - accuracy: 0.8483 - val_loss: 0.7426 - val_accuracy: 0.8452\n",
      "Epoch 131/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7266 - accuracy: 0.8489 - val_loss: 0.7384 - val_accuracy: 0.8454\n",
      "Epoch 132/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7224 - accuracy: 0.8492 - val_loss: 0.7344 - val_accuracy: 0.8456\n",
      "Epoch 133/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7183 - accuracy: 0.8495 - val_loss: 0.7304 - val_accuracy: 0.8464\n",
      "Epoch 134/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7143 - accuracy: 0.8499 - val_loss: 0.7265 - val_accuracy: 0.8466\n",
      "Epoch 135/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7104 - accuracy: 0.8503 - val_loss: 0.7226 - val_accuracy: 0.8468\n",
      "Epoch 136/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7065 - accuracy: 0.8507 - val_loss: 0.7188 - val_accuracy: 0.8474\n",
      "Epoch 137/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7027 - accuracy: 0.8513 - val_loss: 0.7150 - val_accuracy: 0.8474\n",
      "Epoch 138/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6989 - accuracy: 0.8516 - val_loss: 0.7113 - val_accuracy: 0.8480\n",
      "Epoch 139/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6952 - accuracy: 0.8520 - val_loss: 0.7077 - val_accuracy: 0.8482\n",
      "Epoch 140/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6915 - accuracy: 0.8524 - val_loss: 0.7041 - val_accuracy: 0.8484\n",
      "Epoch 141/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6879 - accuracy: 0.8529 - val_loss: 0.7006 - val_accuracy: 0.8480\n",
      "Epoch 142/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6844 - accuracy: 0.8532 - val_loss: 0.6972 - val_accuracy: 0.8486\n",
      "Epoch 143/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6809 - accuracy: 0.8535 - val_loss: 0.6937 - val_accuracy: 0.8490\n",
      "Epoch 144/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6775 - accuracy: 0.8540 - val_loss: 0.6904 - val_accuracy: 0.8492\n",
      "Epoch 145/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6741 - accuracy: 0.8544 - val_loss: 0.6871 - val_accuracy: 0.8498\n",
      "Epoch 146/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6708 - accuracy: 0.8547 - val_loss: 0.6838 - val_accuracy: 0.8496\n",
      "Epoch 147/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6675 - accuracy: 0.8549 - val_loss: 0.6806 - val_accuracy: 0.8496\n",
      "Epoch 148/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6642 - accuracy: 0.8555 - val_loss: 0.6774 - val_accuracy: 0.8502\n",
      "Epoch 149/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6611 - accuracy: 0.8557 - val_loss: 0.6743 - val_accuracy: 0.8504\n",
      "Epoch 150/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6579 - accuracy: 0.8562 - val_loss: 0.6712 - val_accuracy: 0.8506\n",
      "Epoch 151/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6548 - accuracy: 0.8564 - val_loss: 0.6682 - val_accuracy: 0.8508\n",
      "Epoch 152/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6518 - accuracy: 0.8569 - val_loss: 0.6652 - val_accuracy: 0.8510\n",
      "Epoch 153/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6488 - accuracy: 0.8572 - val_loss: 0.6623 - val_accuracy: 0.8512\n",
      "Epoch 154/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6458 - accuracy: 0.8575 - val_loss: 0.6594 - val_accuracy: 0.8514\n",
      "Epoch 155/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6429 - accuracy: 0.8579 - val_loss: 0.6565 - val_accuracy: 0.8520\n",
      "Epoch 156/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6400 - accuracy: 0.8581 - val_loss: 0.6537 - val_accuracy: 0.8520\n",
      "Epoch 157/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6372 - accuracy: 0.8586 - val_loss: 0.6509 - val_accuracy: 0.8520\n",
      "Epoch 158/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6344 - accuracy: 0.8588 - val_loss: 0.6482 - val_accuracy: 0.8522\n",
      "Epoch 159/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6316 - accuracy: 0.8590 - val_loss: 0.6455 - val_accuracy: 0.8524\n",
      "Epoch 160/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6289 - accuracy: 0.8595 - val_loss: 0.6428 - val_accuracy: 0.8526\n",
      "Epoch 161/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6262 - accuracy: 0.8599 - val_loss: 0.6402 - val_accuracy: 0.8532\n",
      "Epoch 162/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6235 - accuracy: 0.8602 - val_loss: 0.6376 - val_accuracy: 0.8538\n",
      "Epoch 163/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6209 - accuracy: 0.8606 - val_loss: 0.6350 - val_accuracy: 0.8536\n",
      "Epoch 164/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6183 - accuracy: 0.8609 - val_loss: 0.6325 - val_accuracy: 0.8540\n",
      "Epoch 165/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6158 - accuracy: 0.8613 - val_loss: 0.6300 - val_accuracy: 0.8546\n",
      "Epoch 166/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6133 - accuracy: 0.8616 - val_loss: 0.6276 - val_accuracy: 0.8544\n",
      "Epoch 167/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6108 - accuracy: 0.8618 - val_loss: 0.6251 - val_accuracy: 0.8544\n",
      "Epoch 168/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6084 - accuracy: 0.8622 - val_loss: 0.6227 - val_accuracy: 0.8550\n",
      "Epoch 169/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6060 - accuracy: 0.8625 - val_loss: 0.6204 - val_accuracy: 0.8552\n",
      "Epoch 170/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6036 - accuracy: 0.8626 - val_loss: 0.6180 - val_accuracy: 0.8556\n",
      "Epoch 171/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6012 - accuracy: 0.8629 - val_loss: 0.6157 - val_accuracy: 0.8560\n",
      "Epoch 172/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5989 - accuracy: 0.8631 - val_loss: 0.6135 - val_accuracy: 0.8562\n",
      "Epoch 173/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5966 - accuracy: 0.8635 - val_loss: 0.6112 - val_accuracy: 0.8568\n",
      "Epoch 174/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5944 - accuracy: 0.8637 - val_loss: 0.6090 - val_accuracy: 0.8570\n",
      "Epoch 175/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5921 - accuracy: 0.8640 - val_loss: 0.6068 - val_accuracy: 0.8576\n",
      "Epoch 176/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5899 - accuracy: 0.8642 - val_loss: 0.6047 - val_accuracy: 0.8584\n",
      "Epoch 177/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5877 - accuracy: 0.8645 - val_loss: 0.6025 - val_accuracy: 0.8590\n",
      "Epoch 178/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5856 - accuracy: 0.8649 - val_loss: 0.6004 - val_accuracy: 0.8592\n",
      "Epoch 179/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5835 - accuracy: 0.8650 - val_loss: 0.5984 - val_accuracy: 0.8596\n",
      "Epoch 180/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5814 - accuracy: 0.8653 - val_loss: 0.5963 - val_accuracy: 0.8600\n",
      "Epoch 181/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5793 - accuracy: 0.8655 - val_loss: 0.5943 - val_accuracy: 0.8602\n",
      "Epoch 182/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5773 - accuracy: 0.8658 - val_loss: 0.5923 - val_accuracy: 0.8604\n",
      "Epoch 183/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5752 - accuracy: 0.8661 - val_loss: 0.5903 - val_accuracy: 0.8604\n",
      "Epoch 184/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5732 - accuracy: 0.8664 - val_loss: 0.5883 - val_accuracy: 0.8606\n",
      "Epoch 185/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5713 - accuracy: 0.8667 - val_loss: 0.5864 - val_accuracy: 0.8614\n",
      "Epoch 186/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5693 - accuracy: 0.8669 - val_loss: 0.5845 - val_accuracy: 0.8620\n",
      "Epoch 187/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5674 - accuracy: 0.8672 - val_loss: 0.5826 - val_accuracy: 0.8620\n",
      "Epoch 188/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5655 - accuracy: 0.8675 - val_loss: 0.5807 - val_accuracy: 0.8624\n",
      "Epoch 189/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5636 - accuracy: 0.8678 - val_loss: 0.5789 - val_accuracy: 0.8624\n",
      "Epoch 190/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5618 - accuracy: 0.8680 - val_loss: 0.5771 - val_accuracy: 0.8626\n",
      "Epoch 191/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5599 - accuracy: 0.8683 - val_loss: 0.5753 - val_accuracy: 0.8626\n",
      "Epoch 192/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5581 - accuracy: 0.8685 - val_loss: 0.5735 - val_accuracy: 0.8630\n",
      "Epoch 193/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5563 - accuracy: 0.8688 - val_loss: 0.5717 - val_accuracy: 0.8634\n",
      "Epoch 194/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5545 - accuracy: 0.8691 - val_loss: 0.5700 - val_accuracy: 0.8640\n",
      "Epoch 195/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5528 - accuracy: 0.8693 - val_loss: 0.5683 - val_accuracy: 0.8644\n",
      "Epoch 196/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5510 - accuracy: 0.8697 - val_loss: 0.5666 - val_accuracy: 0.8648\n",
      "Epoch 197/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5493 - accuracy: 0.8697 - val_loss: 0.5649 - val_accuracy: 0.8648\n",
      "Epoch 198/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5476 - accuracy: 0.8699 - val_loss: 0.5632 - val_accuracy: 0.8646\n",
      "Epoch 199/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5460 - accuracy: 0.8701 - val_loss: 0.5616 - val_accuracy: 0.8650\n",
      "Epoch 200/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5443 - accuracy: 0.8705 - val_loss: 0.5600 - val_accuracy: 0.8652\n",
      "Epoch 1/200\n",
      "215/215 [==============================] - 2s 6ms/step - loss: 2.2388 - accuracy: 0.2137 - val_loss: 2.1409 - val_accuracy: 0.3598\n",
      "Epoch 2/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 2.0519 - accuracy: 0.5071 - val_loss: 1.9639 - val_accuracy: 0.6234\n",
      "Epoch 3/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.8763 - accuracy: 0.6656 - val_loss: 1.7916 - val_accuracy: 0.6992\n",
      "Epoch 4/200\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 1.7041 - accuracy: 0.7231 - val_loss: 1.6229 - val_accuracy: 0.7362\n",
      "Epoch 5/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.5374 - accuracy: 0.7559 - val_loss: 1.4624 - val_accuracy: 0.7630\n",
      "Epoch 6/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.3816 - accuracy: 0.7772 - val_loss: 1.3155 - val_accuracy: 0.7800\n",
      "Epoch 7/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.2418 - accuracy: 0.7936 - val_loss: 1.1863 - val_accuracy: 0.7996\n",
      "Epoch 8/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.1205 - accuracy: 0.8073 - val_loss: 1.0758 - val_accuracy: 0.8132\n",
      "Epoch 9/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 1.0180 - accuracy: 0.8185 - val_loss: 0.9832 - val_accuracy: 0.8232\n",
      "Epoch 10/200\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.9323 - accuracy: 0.8281 - val_loss: 0.9059 - val_accuracy: 0.8294\n",
      "Epoch 11/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8609 - accuracy: 0.8348 - val_loss: 0.8416 - val_accuracy: 0.8368\n",
      "Epoch 12/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.8013 - accuracy: 0.8411 - val_loss: 0.7878 - val_accuracy: 0.8420\n",
      "Epoch 13/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7513 - accuracy: 0.8458 - val_loss: 0.7426 - val_accuracy: 0.8446\n",
      "Epoch 14/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.7090 - accuracy: 0.8504 - val_loss: 0.7041 - val_accuracy: 0.8482\n",
      "Epoch 15/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6729 - accuracy: 0.8543 - val_loss: 0.6713 - val_accuracy: 0.8494\n",
      "Epoch 16/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6418 - accuracy: 0.8577 - val_loss: 0.6428 - val_accuracy: 0.8518\n",
      "Epoch 17/200\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.6149 - accuracy: 0.8613 - val_loss: 0.6181 - val_accuracy: 0.8556\n",
      "Epoch 18/200\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.5913 - accuracy: 0.8640 - val_loss: 0.5964 - val_accuracy: 0.8598\n",
      "Epoch 19/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5706 - accuracy: 0.8671 - val_loss: 0.5770 - val_accuracy: 0.8620\n",
      "Epoch 20/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5522 - accuracy: 0.8693 - val_loss: 0.5600 - val_accuracy: 0.8642\n",
      "Epoch 21/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5357 - accuracy: 0.8717 - val_loss: 0.5446 - val_accuracy: 0.8670\n",
      "Epoch 22/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5210 - accuracy: 0.8738 - val_loss: 0.5308 - val_accuracy: 0.8694\n",
      "Epoch 23/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.8759 - val_loss: 0.5183 - val_accuracy: 0.8706\n",
      "Epoch 24/200\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4955 - accuracy: 0.8778 - val_loss: 0.5071 - val_accuracy: 0.8738\n",
      "Epoch 25/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4845 - accuracy: 0.8797 - val_loss: 0.4966 - val_accuracy: 0.8756\n",
      "Epoch 26/200\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4744 - accuracy: 0.8806 - val_loss: 0.4869 - val_accuracy: 0.8760\n",
      "Epoch 27/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4651 - accuracy: 0.8822 - val_loss: 0.4781 - val_accuracy: 0.8780\n",
      "Epoch 28/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4565 - accuracy: 0.8837 - val_loss: 0.4701 - val_accuracy: 0.8802\n",
      "Epoch 29/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4485 - accuracy: 0.8849 - val_loss: 0.4623 - val_accuracy: 0.8820\n",
      "Epoch 30/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4412 - accuracy: 0.8865 - val_loss: 0.4551 - val_accuracy: 0.8828\n",
      "Epoch 31/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4343 - accuracy: 0.8872 - val_loss: 0.4487 - val_accuracy: 0.8840\n",
      "Epoch 32/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4278 - accuracy: 0.8886 - val_loss: 0.4425 - val_accuracy: 0.8856\n",
      "Epoch 33/200\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4218 - accuracy: 0.8901 - val_loss: 0.4367 - val_accuracy: 0.8852\n",
      "Epoch 34/200\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4161 - accuracy: 0.8908 - val_loss: 0.4313 - val_accuracy: 0.8868\n",
      "Epoch 35/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4108 - accuracy: 0.8919 - val_loss: 0.4261 - val_accuracy: 0.8882\n",
      "Epoch 36/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4057 - accuracy: 0.8929 - val_loss: 0.4213 - val_accuracy: 0.8890\n",
      "Epoch 37/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4010 - accuracy: 0.8939 - val_loss: 0.4165 - val_accuracy: 0.8892\n",
      "Epoch 38/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3964 - accuracy: 0.8947 - val_loss: 0.4123 - val_accuracy: 0.8900\n",
      "Epoch 39/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3921 - accuracy: 0.8951 - val_loss: 0.4080 - val_accuracy: 0.8902\n",
      "Epoch 40/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3880 - accuracy: 0.8961 - val_loss: 0.4041 - val_accuracy: 0.8928\n",
      "Epoch 41/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3841 - accuracy: 0.8969 - val_loss: 0.4004 - val_accuracy: 0.8944\n",
      "Epoch 42/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3804 - accuracy: 0.8973 - val_loss: 0.3967 - val_accuracy: 0.8950\n",
      "Epoch 43/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3768 - accuracy: 0.8983 - val_loss: 0.3933 - val_accuracy: 0.8964\n",
      "Epoch 44/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3734 - accuracy: 0.8987 - val_loss: 0.3898 - val_accuracy: 0.8972\n",
      "Epoch 45/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3702 - accuracy: 0.8997 - val_loss: 0.3867 - val_accuracy: 0.8968\n",
      "Epoch 46/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3670 - accuracy: 0.9002 - val_loss: 0.3836 - val_accuracy: 0.8968\n",
      "Epoch 47/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3640 - accuracy: 0.9010 - val_loss: 0.3807 - val_accuracy: 0.8974\n",
      "Epoch 48/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3611 - accuracy: 0.9014 - val_loss: 0.3779 - val_accuracy: 0.8984\n",
      "Epoch 49/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3583 - accuracy: 0.9021 - val_loss: 0.3752 - val_accuracy: 0.8992\n",
      "Epoch 50/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3556 - accuracy: 0.9024 - val_loss: 0.3725 - val_accuracy: 0.8994\n",
      "Epoch 51/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3530 - accuracy: 0.9030 - val_loss: 0.3701 - val_accuracy: 0.9006\n",
      "Epoch 52/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3505 - accuracy: 0.9035 - val_loss: 0.3677 - val_accuracy: 0.9008\n",
      "Epoch 53/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3480 - accuracy: 0.9039 - val_loss: 0.3652 - val_accuracy: 0.9012\n",
      "Epoch 54/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3457 - accuracy: 0.9044 - val_loss: 0.3629 - val_accuracy: 0.9016\n",
      "Epoch 55/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3434 - accuracy: 0.9048 - val_loss: 0.3606 - val_accuracy: 0.9020\n",
      "Epoch 56/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3412 - accuracy: 0.9054 - val_loss: 0.3585 - val_accuracy: 0.9026\n",
      "Epoch 57/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3390 - accuracy: 0.9059 - val_loss: 0.3564 - val_accuracy: 0.9034\n",
      "Epoch 58/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3369 - accuracy: 0.9065 - val_loss: 0.3545 - val_accuracy: 0.9044\n",
      "Epoch 59/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3349 - accuracy: 0.9069 - val_loss: 0.3525 - val_accuracy: 0.9048\n",
      "Epoch 60/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3329 - accuracy: 0.9075 - val_loss: 0.3505 - val_accuracy: 0.9054\n",
      "Epoch 61/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3310 - accuracy: 0.9080 - val_loss: 0.3487 - val_accuracy: 0.9058\n",
      "Epoch 62/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3291 - accuracy: 0.9083 - val_loss: 0.3467 - val_accuracy: 0.9064\n",
      "Epoch 63/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3273 - accuracy: 0.9088 - val_loss: 0.3450 - val_accuracy: 0.9066\n",
      "Epoch 64/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3255 - accuracy: 0.9091 - val_loss: 0.3433 - val_accuracy: 0.9076\n",
      "Epoch 65/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3238 - accuracy: 0.9095 - val_loss: 0.3415 - val_accuracy: 0.9072\n",
      "Epoch 66/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3221 - accuracy: 0.9099 - val_loss: 0.3399 - val_accuracy: 0.9074\n",
      "Epoch 67/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3204 - accuracy: 0.9103 - val_loss: 0.3383 - val_accuracy: 0.9078\n",
      "Epoch 68/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3188 - accuracy: 0.9107 - val_loss: 0.3368 - val_accuracy: 0.9086\n",
      "Epoch 69/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3172 - accuracy: 0.9109 - val_loss: 0.3352 - val_accuracy: 0.9086\n",
      "Epoch 70/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3156 - accuracy: 0.9115 - val_loss: 0.3337 - val_accuracy: 0.9082\n",
      "Epoch 71/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3141 - accuracy: 0.9117 - val_loss: 0.3322 - val_accuracy: 0.9086\n",
      "Epoch 72/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3126 - accuracy: 0.9122 - val_loss: 0.3309 - val_accuracy: 0.9092\n",
      "Epoch 73/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3111 - accuracy: 0.9130 - val_loss: 0.3294 - val_accuracy: 0.9094\n",
      "Epoch 74/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3097 - accuracy: 0.9133 - val_loss: 0.3280 - val_accuracy: 0.9100\n",
      "Epoch 75/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3083 - accuracy: 0.9136 - val_loss: 0.3267 - val_accuracy: 0.9102\n",
      "Epoch 76/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3069 - accuracy: 0.9142 - val_loss: 0.3254 - val_accuracy: 0.9104\n",
      "Epoch 77/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3055 - accuracy: 0.9145 - val_loss: 0.3241 - val_accuracy: 0.9108\n",
      "Epoch 78/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3042 - accuracy: 0.9148 - val_loss: 0.3227 - val_accuracy: 0.9108\n",
      "Epoch 79/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3029 - accuracy: 0.9153 - val_loss: 0.3215 - val_accuracy: 0.9114\n",
      "Epoch 80/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3016 - accuracy: 0.9155 - val_loss: 0.3202 - val_accuracy: 0.9112\n",
      "Epoch 81/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3003 - accuracy: 0.9161 - val_loss: 0.3190 - val_accuracy: 0.9114\n",
      "Epoch 82/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2991 - accuracy: 0.9161 - val_loss: 0.3177 - val_accuracy: 0.9120\n",
      "Epoch 83/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2979 - accuracy: 0.9165 - val_loss: 0.3167 - val_accuracy: 0.9124\n",
      "Epoch 84/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2966 - accuracy: 0.9168 - val_loss: 0.3153 - val_accuracy: 0.9128\n",
      "Epoch 85/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2955 - accuracy: 0.9173 - val_loss: 0.3145 - val_accuracy: 0.9126\n",
      "Epoch 86/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2943 - accuracy: 0.9177 - val_loss: 0.3133 - val_accuracy: 0.9132\n",
      "Epoch 87/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2932 - accuracy: 0.9178 - val_loss: 0.3122 - val_accuracy: 0.9124\n",
      "Epoch 88/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2920 - accuracy: 0.9180 - val_loss: 0.3113 - val_accuracy: 0.9122\n",
      "Epoch 89/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2909 - accuracy: 0.9186 - val_loss: 0.3103 - val_accuracy: 0.9134\n",
      "Epoch 90/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2898 - accuracy: 0.9190 - val_loss: 0.3088 - val_accuracy: 0.9150\n",
      "Epoch 91/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2887 - accuracy: 0.9188 - val_loss: 0.3079 - val_accuracy: 0.9156\n",
      "Epoch 92/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2877 - accuracy: 0.9193 - val_loss: 0.3069 - val_accuracy: 0.9142\n",
      "Epoch 93/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2866 - accuracy: 0.9195 - val_loss: 0.3059 - val_accuracy: 0.9150\n",
      "Epoch 94/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2855 - accuracy: 0.9197 - val_loss: 0.3048 - val_accuracy: 0.9154\n",
      "Epoch 95/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2846 - accuracy: 0.9200 - val_loss: 0.3039 - val_accuracy: 0.9152\n",
      "Epoch 96/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2835 - accuracy: 0.9203 - val_loss: 0.3031 - val_accuracy: 0.9158\n",
      "Epoch 97/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2825 - accuracy: 0.9206 - val_loss: 0.3020 - val_accuracy: 0.9156\n",
      "Epoch 98/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2815 - accuracy: 0.9208 - val_loss: 0.3009 - val_accuracy: 0.9162\n",
      "Epoch 99/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2806 - accuracy: 0.9209 - val_loss: 0.3002 - val_accuracy: 0.9162\n",
      "Epoch 100/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2796 - accuracy: 0.9214 - val_loss: 0.2992 - val_accuracy: 0.9166\n",
      "Epoch 101/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2786 - accuracy: 0.9217 - val_loss: 0.2985 - val_accuracy: 0.9164\n",
      "Epoch 102/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2777 - accuracy: 0.9218 - val_loss: 0.2974 - val_accuracy: 0.9170\n",
      "Epoch 103/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2768 - accuracy: 0.9221 - val_loss: 0.2967 - val_accuracy: 0.9168\n",
      "Epoch 104/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2759 - accuracy: 0.9223 - val_loss: 0.2958 - val_accuracy: 0.9164\n",
      "Epoch 105/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2749 - accuracy: 0.9226 - val_loss: 0.2949 - val_accuracy: 0.9170\n",
      "Epoch 106/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2740 - accuracy: 0.9228 - val_loss: 0.2940 - val_accuracy: 0.9174\n",
      "Epoch 107/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2732 - accuracy: 0.9231 - val_loss: 0.2933 - val_accuracy: 0.9174\n",
      "Epoch 108/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2723 - accuracy: 0.9233 - val_loss: 0.2923 - val_accuracy: 0.9178\n",
      "Epoch 109/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2714 - accuracy: 0.9235 - val_loss: 0.2915 - val_accuracy: 0.9180\n",
      "Epoch 110/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2706 - accuracy: 0.9238 - val_loss: 0.2908 - val_accuracy: 0.9182\n",
      "Epoch 111/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2697 - accuracy: 0.9238 - val_loss: 0.2900 - val_accuracy: 0.9188\n",
      "Epoch 112/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2688 - accuracy: 0.9243 - val_loss: 0.2891 - val_accuracy: 0.9184\n",
      "Epoch 113/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2680 - accuracy: 0.9244 - val_loss: 0.2883 - val_accuracy: 0.9188\n",
      "Epoch 114/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2672 - accuracy: 0.9247 - val_loss: 0.2875 - val_accuracy: 0.9186\n",
      "Epoch 115/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2664 - accuracy: 0.9250 - val_loss: 0.2868 - val_accuracy: 0.9198\n",
      "Epoch 116/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2656 - accuracy: 0.9252 - val_loss: 0.2859 - val_accuracy: 0.9192\n",
      "Epoch 117/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2647 - accuracy: 0.9255 - val_loss: 0.2854 - val_accuracy: 0.9200\n",
      "Epoch 118/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2640 - accuracy: 0.9257 - val_loss: 0.2846 - val_accuracy: 0.9200\n",
      "Epoch 119/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2632 - accuracy: 0.9259 - val_loss: 0.2839 - val_accuracy: 0.9196\n",
      "Epoch 120/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2624 - accuracy: 0.9261 - val_loss: 0.2831 - val_accuracy: 0.9202\n",
      "Epoch 121/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2616 - accuracy: 0.9263 - val_loss: 0.2823 - val_accuracy: 0.9208\n",
      "Epoch 122/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2609 - accuracy: 0.9265 - val_loss: 0.2817 - val_accuracy: 0.9204\n",
      "Epoch 123/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2601 - accuracy: 0.9268 - val_loss: 0.2809 - val_accuracy: 0.9210\n",
      "Epoch 124/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2593 - accuracy: 0.9270 - val_loss: 0.2801 - val_accuracy: 0.9214\n",
      "Epoch 125/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2586 - accuracy: 0.9272 - val_loss: 0.2795 - val_accuracy: 0.9222\n",
      "Epoch 126/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2578 - accuracy: 0.9274 - val_loss: 0.2789 - val_accuracy: 0.9212\n",
      "Epoch 127/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2571 - accuracy: 0.9275 - val_loss: 0.2782 - val_accuracy: 0.9220\n",
      "Epoch 128/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2563 - accuracy: 0.9279 - val_loss: 0.2776 - val_accuracy: 0.9216\n",
      "Epoch 129/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2556 - accuracy: 0.9280 - val_loss: 0.2768 - val_accuracy: 0.9226\n",
      "Epoch 130/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2549 - accuracy: 0.9282 - val_loss: 0.2762 - val_accuracy: 0.9230\n",
      "Epoch 131/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2542 - accuracy: 0.9285 - val_loss: 0.2756 - val_accuracy: 0.9228\n",
      "Epoch 132/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2535 - accuracy: 0.9287 - val_loss: 0.2749 - val_accuracy: 0.9230\n",
      "Epoch 133/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2528 - accuracy: 0.9288 - val_loss: 0.2742 - val_accuracy: 0.9224\n",
      "Epoch 134/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2521 - accuracy: 0.9294 - val_loss: 0.2736 - val_accuracy: 0.9228\n",
      "Epoch 135/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2514 - accuracy: 0.9295 - val_loss: 0.2729 - val_accuracy: 0.9230\n",
      "Epoch 136/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2507 - accuracy: 0.9297 - val_loss: 0.2722 - val_accuracy: 0.9228\n",
      "Epoch 137/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2500 - accuracy: 0.9298 - val_loss: 0.2716 - val_accuracy: 0.9228\n",
      "Epoch 138/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2494 - accuracy: 0.9299 - val_loss: 0.2711 - val_accuracy: 0.9236\n",
      "Epoch 139/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2486 - accuracy: 0.9302 - val_loss: 0.2703 - val_accuracy: 0.9234\n",
      "Epoch 140/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2480 - accuracy: 0.9303 - val_loss: 0.2697 - val_accuracy: 0.9232\n",
      "Epoch 141/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2474 - accuracy: 0.9305 - val_loss: 0.2691 - val_accuracy: 0.9234\n",
      "Epoch 142/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2467 - accuracy: 0.9306 - val_loss: 0.2685 - val_accuracy: 0.9240\n",
      "Epoch 143/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2461 - accuracy: 0.9308 - val_loss: 0.2680 - val_accuracy: 0.9236\n",
      "Epoch 144/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2454 - accuracy: 0.9310 - val_loss: 0.2674 - val_accuracy: 0.9242\n",
      "Epoch 145/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2447 - accuracy: 0.9312 - val_loss: 0.2666 - val_accuracy: 0.9240\n",
      "Epoch 146/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2441 - accuracy: 0.9314 - val_loss: 0.2661 - val_accuracy: 0.9246\n",
      "Epoch 147/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2435 - accuracy: 0.9315 - val_loss: 0.2655 - val_accuracy: 0.9244\n",
      "Epoch 148/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2429 - accuracy: 0.9315 - val_loss: 0.2649 - val_accuracy: 0.9246\n",
      "Epoch 149/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2422 - accuracy: 0.9315 - val_loss: 0.2644 - val_accuracy: 0.9250\n",
      "Epoch 150/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2416 - accuracy: 0.9320 - val_loss: 0.2638 - val_accuracy: 0.9250\n",
      "Epoch 151/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2410 - accuracy: 0.9321 - val_loss: 0.2634 - val_accuracy: 0.9258\n",
      "Epoch 152/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2404 - accuracy: 0.9320 - val_loss: 0.2628 - val_accuracy: 0.9254\n",
      "Epoch 153/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2398 - accuracy: 0.9325 - val_loss: 0.2621 - val_accuracy: 0.9256\n",
      "Epoch 154/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2392 - accuracy: 0.9326 - val_loss: 0.2616 - val_accuracy: 0.9258\n",
      "Epoch 155/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2386 - accuracy: 0.9327 - val_loss: 0.2611 - val_accuracy: 0.9258\n",
      "Epoch 156/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2380 - accuracy: 0.9329 - val_loss: 0.2606 - val_accuracy: 0.9264\n",
      "Epoch 157/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2374 - accuracy: 0.9330 - val_loss: 0.2600 - val_accuracy: 0.9258\n",
      "Epoch 158/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2368 - accuracy: 0.9334 - val_loss: 0.2594 - val_accuracy: 0.9268\n",
      "Epoch 159/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2362 - accuracy: 0.9336 - val_loss: 0.2589 - val_accuracy: 0.9278\n",
      "Epoch 160/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2356 - accuracy: 0.9338 - val_loss: 0.2584 - val_accuracy: 0.9266\n",
      "Epoch 161/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2350 - accuracy: 0.9340 - val_loss: 0.2578 - val_accuracy: 0.9272\n",
      "Epoch 162/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2344 - accuracy: 0.9343 - val_loss: 0.2572 - val_accuracy: 0.9280\n",
      "Epoch 163/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2339 - accuracy: 0.9343 - val_loss: 0.2568 - val_accuracy: 0.9282\n",
      "Epoch 164/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2333 - accuracy: 0.9345 - val_loss: 0.2561 - val_accuracy: 0.9272\n",
      "Epoch 165/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.2327 - accuracy: 0.9347 - val_loss: 0.2557 - val_accuracy: 0.9280\n",
      "Epoch 166/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2321 - accuracy: 0.9347 - val_loss: 0.2551 - val_accuracy: 0.9282\n",
      "Epoch 167/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2316 - accuracy: 0.9350 - val_loss: 0.2547 - val_accuracy: 0.9284\n",
      "Epoch 168/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.2310 - accuracy: 0.9352 - val_loss: 0.2542 - val_accuracy: 0.9280\n",
      "Epoch 169/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2305 - accuracy: 0.9350 - val_loss: 0.2536 - val_accuracy: 0.9280\n",
      "Epoch 170/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2299 - accuracy: 0.9356 - val_loss: 0.2532 - val_accuracy: 0.9288\n",
      "Epoch 171/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2294 - accuracy: 0.9359 - val_loss: 0.2526 - val_accuracy: 0.9288\n",
      "Epoch 172/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2288 - accuracy: 0.9358 - val_loss: 0.2520 - val_accuracy: 0.9292\n",
      "Epoch 173/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2283 - accuracy: 0.9360 - val_loss: 0.2517 - val_accuracy: 0.9292\n",
      "Epoch 174/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2277 - accuracy: 0.9362 - val_loss: 0.2512 - val_accuracy: 0.9290\n",
      "Epoch 175/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2272 - accuracy: 0.9365 - val_loss: 0.2506 - val_accuracy: 0.9290\n",
      "Epoch 176/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2267 - accuracy: 0.9364 - val_loss: 0.2501 - val_accuracy: 0.9292\n",
      "Epoch 177/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2261 - accuracy: 0.9369 - val_loss: 0.2496 - val_accuracy: 0.9298\n",
      "Epoch 178/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2256 - accuracy: 0.9370 - val_loss: 0.2490 - val_accuracy: 0.9292\n",
      "Epoch 179/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2251 - accuracy: 0.9369 - val_loss: 0.2487 - val_accuracy: 0.9300\n",
      "Epoch 180/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2246 - accuracy: 0.9369 - val_loss: 0.2482 - val_accuracy: 0.9294\n",
      "Epoch 181/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2240 - accuracy: 0.9372 - val_loss: 0.2478 - val_accuracy: 0.9302\n",
      "Epoch 182/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2235 - accuracy: 0.9373 - val_loss: 0.2474 - val_accuracy: 0.9304\n",
      "Epoch 183/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2230 - accuracy: 0.9373 - val_loss: 0.2469 - val_accuracy: 0.9302\n",
      "Epoch 184/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2225 - accuracy: 0.9377 - val_loss: 0.2463 - val_accuracy: 0.9302\n",
      "Epoch 185/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2220 - accuracy: 0.9378 - val_loss: 0.2459 - val_accuracy: 0.9300\n",
      "Epoch 186/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2215 - accuracy: 0.9379 - val_loss: 0.2453 - val_accuracy: 0.9298\n",
      "Epoch 187/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2210 - accuracy: 0.9380 - val_loss: 0.2449 - val_accuracy: 0.9304\n",
      "Epoch 188/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2205 - accuracy: 0.9383 - val_loss: 0.2445 - val_accuracy: 0.9302\n",
      "Epoch 189/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2200 - accuracy: 0.9383 - val_loss: 0.2440 - val_accuracy: 0.9302\n",
      "Epoch 190/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2195 - accuracy: 0.9387 - val_loss: 0.2437 - val_accuracy: 0.9310\n",
      "Epoch 191/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2189 - accuracy: 0.9387 - val_loss: 0.2430 - val_accuracy: 0.9312\n",
      "Epoch 192/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2185 - accuracy: 0.9385 - val_loss: 0.2428 - val_accuracy: 0.9310\n",
      "Epoch 193/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2180 - accuracy: 0.9387 - val_loss: 0.2422 - val_accuracy: 0.9308\n",
      "Epoch 194/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2175 - accuracy: 0.9391 - val_loss: 0.2418 - val_accuracy: 0.9306\n",
      "Epoch 195/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2170 - accuracy: 0.9390 - val_loss: 0.2415 - val_accuracy: 0.9310\n",
      "Epoch 196/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2165 - accuracy: 0.9395 - val_loss: 0.2408 - val_accuracy: 0.9312\n",
      "Epoch 197/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2160 - accuracy: 0.9395 - val_loss: 0.2406 - val_accuracy: 0.9310\n",
      "Epoch 198/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2156 - accuracy: 0.9397 - val_loss: 0.2401 - val_accuracy: 0.9310\n",
      "Epoch 199/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2151 - accuracy: 0.9400 - val_loss: 0.2396 - val_accuracy: 0.9312\n",
      "Epoch 200/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2146 - accuracy: 0.9400 - val_loss: 0.2392 - val_accuracy: 0.9312\n",
      "Epoch 1/200\n",
      "215/215 [==============================] - 2s 6ms/step - loss: 1.5153 - accuracy: 0.6860 - val_loss: 0.9063 - val_accuracy: 0.8272\n",
      "Epoch 2/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.6784 - accuracy: 0.8533 - val_loss: 0.5615 - val_accuracy: 0.8620\n",
      "Epoch 3/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4847 - accuracy: 0.8785 - val_loss: 0.4563 - val_accuracy: 0.8834\n",
      "Epoch 4/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4110 - accuracy: 0.8914 - val_loss: 0.4047 - val_accuracy: 0.8932\n",
      "Epoch 5/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3706 - accuracy: 0.8989 - val_loss: 0.3733 - val_accuracy: 0.8994\n",
      "Epoch 6/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3439 - accuracy: 0.9047 - val_loss: 0.3518 - val_accuracy: 0.9042\n",
      "Epoch 7/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3244 - accuracy: 0.9092 - val_loss: 0.3345 - val_accuracy: 0.9090\n",
      "Epoch 8/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3092 - accuracy: 0.9132 - val_loss: 0.3208 - val_accuracy: 0.9102\n",
      "Epoch 9/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2963 - accuracy: 0.9170 - val_loss: 0.3096 - val_accuracy: 0.9132\n",
      "Epoch 10/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2854 - accuracy: 0.9196 - val_loss: 0.3007 - val_accuracy: 0.9164\n",
      "Epoch 11/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2757 - accuracy: 0.9225 - val_loss: 0.2922 - val_accuracy: 0.9172\n",
      "Epoch 12/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2674 - accuracy: 0.9245 - val_loss: 0.2836 - val_accuracy: 0.9210\n",
      "Epoch 13/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2596 - accuracy: 0.9269 - val_loss: 0.2762 - val_accuracy: 0.9212\n",
      "Epoch 14/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2523 - accuracy: 0.9287 - val_loss: 0.2710 - val_accuracy: 0.9238\n",
      "Epoch 15/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2458 - accuracy: 0.9313 - val_loss: 0.2646 - val_accuracy: 0.9262\n",
      "Epoch 16/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2396 - accuracy: 0.9324 - val_loss: 0.2583 - val_accuracy: 0.9272\n",
      "Epoch 17/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2336 - accuracy: 0.9343 - val_loss: 0.2529 - val_accuracy: 0.9288\n",
      "Epoch 18/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2281 - accuracy: 0.9357 - val_loss: 0.2484 - val_accuracy: 0.9300\n",
      "Epoch 19/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2229 - accuracy: 0.9373 - val_loss: 0.2444 - val_accuracy: 0.9308\n",
      "Epoch 20/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2181 - accuracy: 0.9386 - val_loss: 0.2392 - val_accuracy: 0.9312\n",
      "Epoch 21/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2133 - accuracy: 0.9401 - val_loss: 0.2353 - val_accuracy: 0.9312\n",
      "Epoch 22/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2087 - accuracy: 0.9413 - val_loss: 0.2305 - val_accuracy: 0.9340\n",
      "Epoch 23/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2042 - accuracy: 0.9429 - val_loss: 0.2274 - val_accuracy: 0.9346\n",
      "Epoch 24/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2001 - accuracy: 0.9438 - val_loss: 0.2239 - val_accuracy: 0.9348\n",
      "Epoch 25/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1961 - accuracy: 0.9451 - val_loss: 0.2198 - val_accuracy: 0.9368\n",
      "Epoch 26/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1923 - accuracy: 0.9459 - val_loss: 0.2163 - val_accuracy: 0.9378\n",
      "Epoch 27/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1884 - accuracy: 0.9469 - val_loss: 0.2134 - val_accuracy: 0.9390\n",
      "Epoch 28/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1848 - accuracy: 0.9481 - val_loss: 0.2113 - val_accuracy: 0.9400\n",
      "Epoch 29/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1814 - accuracy: 0.9487 - val_loss: 0.2069 - val_accuracy: 0.9408\n",
      "Epoch 30/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1778 - accuracy: 0.9499 - val_loss: 0.2041 - val_accuracy: 0.9424\n",
      "Epoch 31/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1746 - accuracy: 0.9513 - val_loss: 0.2016 - val_accuracy: 0.9434\n",
      "Epoch 32/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1714 - accuracy: 0.9521 - val_loss: 0.1987 - val_accuracy: 0.9436\n",
      "Epoch 33/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1685 - accuracy: 0.9524 - val_loss: 0.1962 - val_accuracy: 0.9448\n",
      "Epoch 34/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1654 - accuracy: 0.9533 - val_loss: 0.1937 - val_accuracy: 0.9454\n",
      "Epoch 35/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1625 - accuracy: 0.9541 - val_loss: 0.1911 - val_accuracy: 0.9464\n",
      "Epoch 36/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1596 - accuracy: 0.9548 - val_loss: 0.1882 - val_accuracy: 0.9462\n",
      "Epoch 37/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1570 - accuracy: 0.9555 - val_loss: 0.1855 - val_accuracy: 0.9476\n",
      "Epoch 38/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1544 - accuracy: 0.9568 - val_loss: 0.1843 - val_accuracy: 0.9468\n",
      "Epoch 39/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1519 - accuracy: 0.9573 - val_loss: 0.1811 - val_accuracy: 0.9490\n",
      "Epoch 40/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1493 - accuracy: 0.9581 - val_loss: 0.1797 - val_accuracy: 0.9484\n",
      "Epoch 41/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1469 - accuracy: 0.9585 - val_loss: 0.1766 - val_accuracy: 0.9494\n",
      "Epoch 42/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1447 - accuracy: 0.9594 - val_loss: 0.1751 - val_accuracy: 0.9498\n",
      "Epoch 43/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1423 - accuracy: 0.9596 - val_loss: 0.1728 - val_accuracy: 0.9504\n",
      "Epoch 44/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1400 - accuracy: 0.9603 - val_loss: 0.1717 - val_accuracy: 0.9494\n",
      "Epoch 45/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1379 - accuracy: 0.9611 - val_loss: 0.1693 - val_accuracy: 0.9512\n",
      "Epoch 46/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1358 - accuracy: 0.9617 - val_loss: 0.1674 - val_accuracy: 0.9522\n",
      "Epoch 47/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1335 - accuracy: 0.9625 - val_loss: 0.1656 - val_accuracy: 0.9520\n",
      "Epoch 48/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1316 - accuracy: 0.9629 - val_loss: 0.1644 - val_accuracy: 0.9530\n",
      "Epoch 49/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1297 - accuracy: 0.9632 - val_loss: 0.1631 - val_accuracy: 0.9522\n",
      "Epoch 50/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1279 - accuracy: 0.9640 - val_loss: 0.1611 - val_accuracy: 0.9542\n",
      "Epoch 51/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1259 - accuracy: 0.9647 - val_loss: 0.1593 - val_accuracy: 0.9548\n",
      "Epoch 52/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1241 - accuracy: 0.9651 - val_loss: 0.1585 - val_accuracy: 0.9536\n",
      "Epoch 53/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1224 - accuracy: 0.9657 - val_loss: 0.1568 - val_accuracy: 0.9554\n",
      "Epoch 54/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1207 - accuracy: 0.9660 - val_loss: 0.1548 - val_accuracy: 0.9562\n",
      "Epoch 55/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1190 - accuracy: 0.9667 - val_loss: 0.1529 - val_accuracy: 0.9562\n",
      "Epoch 56/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1173 - accuracy: 0.9670 - val_loss: 0.1519 - val_accuracy: 0.9564\n",
      "Epoch 57/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1158 - accuracy: 0.9674 - val_loss: 0.1506 - val_accuracy: 0.9562\n",
      "Epoch 58/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1142 - accuracy: 0.9679 - val_loss: 0.1500 - val_accuracy: 0.9566\n",
      "Epoch 59/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1126 - accuracy: 0.9686 - val_loss: 0.1490 - val_accuracy: 0.9564\n",
      "Epoch 60/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1111 - accuracy: 0.9691 - val_loss: 0.1466 - val_accuracy: 0.9576\n",
      "Epoch 61/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1097 - accuracy: 0.9694 - val_loss: 0.1455 - val_accuracy: 0.9586\n",
      "Epoch 62/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1082 - accuracy: 0.9698 - val_loss: 0.1451 - val_accuracy: 0.9576\n",
      "Epoch 63/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1068 - accuracy: 0.9702 - val_loss: 0.1442 - val_accuracy: 0.9574\n",
      "Epoch 64/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1055 - accuracy: 0.9709 - val_loss: 0.1420 - val_accuracy: 0.9586\n",
      "Epoch 65/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1042 - accuracy: 0.9714 - val_loss: 0.1410 - val_accuracy: 0.9586\n",
      "Epoch 66/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1028 - accuracy: 0.9714 - val_loss: 0.1403 - val_accuracy: 0.9600\n",
      "Epoch 67/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1015 - accuracy: 0.9718 - val_loss: 0.1389 - val_accuracy: 0.9594\n",
      "Epoch 68/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1002 - accuracy: 0.9725 - val_loss: 0.1377 - val_accuracy: 0.9592\n",
      "Epoch 69/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0990 - accuracy: 0.9732 - val_loss: 0.1371 - val_accuracy: 0.9598\n",
      "Epoch 70/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0978 - accuracy: 0.9734 - val_loss: 0.1362 - val_accuracy: 0.9600\n",
      "Epoch 71/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0966 - accuracy: 0.9735 - val_loss: 0.1352 - val_accuracy: 0.9598\n",
      "Epoch 72/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0954 - accuracy: 0.9738 - val_loss: 0.1350 - val_accuracy: 0.9596\n",
      "Epoch 73/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0942 - accuracy: 0.9742 - val_loss: 0.1332 - val_accuracy: 0.9600\n",
      "Epoch 74/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0931 - accuracy: 0.9744 - val_loss: 0.1324 - val_accuracy: 0.9606\n",
      "Epoch 75/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0921 - accuracy: 0.9745 - val_loss: 0.1325 - val_accuracy: 0.9606\n",
      "Epoch 76/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0909 - accuracy: 0.9753 - val_loss: 0.1306 - val_accuracy: 0.9608\n",
      "Epoch 77/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0899 - accuracy: 0.9753 - val_loss: 0.1300 - val_accuracy: 0.9614\n",
      "Epoch 78/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0888 - accuracy: 0.9759 - val_loss: 0.1292 - val_accuracy: 0.9616\n",
      "Epoch 79/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0878 - accuracy: 0.9759 - val_loss: 0.1281 - val_accuracy: 0.9604\n",
      "Epoch 80/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0867 - accuracy: 0.9762 - val_loss: 0.1277 - val_accuracy: 0.9614\n",
      "Epoch 81/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0858 - accuracy: 0.9766 - val_loss: 0.1267 - val_accuracy: 0.9610\n",
      "Epoch 82/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0847 - accuracy: 0.9769 - val_loss: 0.1266 - val_accuracy: 0.9614\n",
      "Epoch 83/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0837 - accuracy: 0.9769 - val_loss: 0.1251 - val_accuracy: 0.9614\n",
      "Epoch 84/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0828 - accuracy: 0.9773 - val_loss: 0.1245 - val_accuracy: 0.9634\n",
      "Epoch 85/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0819 - accuracy: 0.9778 - val_loss: 0.1238 - val_accuracy: 0.9632\n",
      "Epoch 86/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0809 - accuracy: 0.9779 - val_loss: 0.1231 - val_accuracy: 0.9640\n",
      "Epoch 87/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0800 - accuracy: 0.9781 - val_loss: 0.1228 - val_accuracy: 0.9630\n",
      "Epoch 88/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0791 - accuracy: 0.9784 - val_loss: 0.1223 - val_accuracy: 0.9624\n",
      "Epoch 89/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0782 - accuracy: 0.9787 - val_loss: 0.1209 - val_accuracy: 0.9630\n",
      "Epoch 90/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0774 - accuracy: 0.9789 - val_loss: 0.1207 - val_accuracy: 0.9640\n",
      "Epoch 91/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0766 - accuracy: 0.9790 - val_loss: 0.1198 - val_accuracy: 0.9630\n",
      "Epoch 92/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0757 - accuracy: 0.9795 - val_loss: 0.1194 - val_accuracy: 0.9644\n",
      "Epoch 93/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0748 - accuracy: 0.9798 - val_loss: 0.1189 - val_accuracy: 0.9644\n",
      "Epoch 94/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0740 - accuracy: 0.9799 - val_loss: 0.1186 - val_accuracy: 0.9626\n",
      "Epoch 95/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0733 - accuracy: 0.9802 - val_loss: 0.1173 - val_accuracy: 0.9652\n",
      "Epoch 96/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0724 - accuracy: 0.9805 - val_loss: 0.1170 - val_accuracy: 0.9642\n",
      "Epoch 97/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0716 - accuracy: 0.9805 - val_loss: 0.1163 - val_accuracy: 0.9640\n",
      "Epoch 98/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0709 - accuracy: 0.9808 - val_loss: 0.1159 - val_accuracy: 0.9650\n",
      "Epoch 99/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0702 - accuracy: 0.9811 - val_loss: 0.1154 - val_accuracy: 0.9654\n",
      "Epoch 100/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0694 - accuracy: 0.9814 - val_loss: 0.1148 - val_accuracy: 0.9644\n",
      "Epoch 101/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0687 - accuracy: 0.9815 - val_loss: 0.1140 - val_accuracy: 0.9650\n",
      "Epoch 102/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0679 - accuracy: 0.9818 - val_loss: 0.1135 - val_accuracy: 0.9658\n",
      "Epoch 103/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0672 - accuracy: 0.9818 - val_loss: 0.1138 - val_accuracy: 0.9662\n",
      "Epoch 104/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0665 - accuracy: 0.9824 - val_loss: 0.1130 - val_accuracy: 0.9658\n",
      "Epoch 105/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0659 - accuracy: 0.9823 - val_loss: 0.1124 - val_accuracy: 0.9660\n",
      "Epoch 106/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0651 - accuracy: 0.9826 - val_loss: 0.1116 - val_accuracy: 0.9658\n",
      "Epoch 107/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0644 - accuracy: 0.9829 - val_loss: 0.1114 - val_accuracy: 0.9656\n",
      "Epoch 108/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0637 - accuracy: 0.9831 - val_loss: 0.1107 - val_accuracy: 0.9670\n",
      "Epoch 109/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0631 - accuracy: 0.9832 - val_loss: 0.1099 - val_accuracy: 0.9668\n",
      "Epoch 110/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0625 - accuracy: 0.9834 - val_loss: 0.1104 - val_accuracy: 0.9668\n",
      "Epoch 111/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0618 - accuracy: 0.9837 - val_loss: 0.1094 - val_accuracy: 0.9664\n",
      "Epoch 112/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0612 - accuracy: 0.9836 - val_loss: 0.1089 - val_accuracy: 0.9666\n",
      "Epoch 113/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0606 - accuracy: 0.9839 - val_loss: 0.1087 - val_accuracy: 0.9666\n",
      "Epoch 114/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0599 - accuracy: 0.9842 - val_loss: 0.1081 - val_accuracy: 0.9662\n",
      "Epoch 115/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0593 - accuracy: 0.9843 - val_loss: 0.1077 - val_accuracy: 0.9670\n",
      "Epoch 116/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0587 - accuracy: 0.9845 - val_loss: 0.1071 - val_accuracy: 0.9668\n",
      "Epoch 117/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0581 - accuracy: 0.9846 - val_loss: 0.1073 - val_accuracy: 0.9664\n",
      "Epoch 118/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0576 - accuracy: 0.9850 - val_loss: 0.1061 - val_accuracy: 0.9672\n",
      "Epoch 119/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0570 - accuracy: 0.9849 - val_loss: 0.1059 - val_accuracy: 0.9678\n",
      "Epoch 120/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0563 - accuracy: 0.9852 - val_loss: 0.1064 - val_accuracy: 0.9670\n",
      "Epoch 121/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0559 - accuracy: 0.9853 - val_loss: 0.1057 - val_accuracy: 0.9680\n",
      "Epoch 122/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0553 - accuracy: 0.9855 - val_loss: 0.1053 - val_accuracy: 0.9680\n",
      "Epoch 123/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0548 - accuracy: 0.9857 - val_loss: 0.1048 - val_accuracy: 0.9678\n",
      "Epoch 124/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0542 - accuracy: 0.9859 - val_loss: 0.1043 - val_accuracy: 0.9686\n",
      "Epoch 125/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0536 - accuracy: 0.9860 - val_loss: 0.1043 - val_accuracy: 0.9690\n",
      "Epoch 126/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0531 - accuracy: 0.9862 - val_loss: 0.1039 - val_accuracy: 0.9680\n",
      "Epoch 127/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0526 - accuracy: 0.9865 - val_loss: 0.1040 - val_accuracy: 0.9680\n",
      "Epoch 128/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0521 - accuracy: 0.9864 - val_loss: 0.1030 - val_accuracy: 0.9684\n",
      "Epoch 129/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0516 - accuracy: 0.9868 - val_loss: 0.1035 - val_accuracy: 0.9682\n",
      "Epoch 130/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0511 - accuracy: 0.9868 - val_loss: 0.1027 - val_accuracy: 0.9686\n",
      "Epoch 131/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0506 - accuracy: 0.9869 - val_loss: 0.1021 - val_accuracy: 0.9678\n",
      "Epoch 132/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0501 - accuracy: 0.9871 - val_loss: 0.1023 - val_accuracy: 0.9696\n",
      "Epoch 133/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0497 - accuracy: 0.9874 - val_loss: 0.1022 - val_accuracy: 0.9696\n",
      "Epoch 134/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0492 - accuracy: 0.9875 - val_loss: 0.1015 - val_accuracy: 0.9676\n",
      "Epoch 135/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0487 - accuracy: 0.9875 - val_loss: 0.1016 - val_accuracy: 0.9684\n",
      "Epoch 136/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0482 - accuracy: 0.9877 - val_loss: 0.1009 - val_accuracy: 0.9692\n",
      "Epoch 137/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0478 - accuracy: 0.9878 - val_loss: 0.1004 - val_accuracy: 0.9688\n",
      "Epoch 138/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0473 - accuracy: 0.9881 - val_loss: 0.1003 - val_accuracy: 0.9696\n",
      "Epoch 139/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0469 - accuracy: 0.9881 - val_loss: 0.1005 - val_accuracy: 0.9694\n",
      "Epoch 140/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0465 - accuracy: 0.9883 - val_loss: 0.1002 - val_accuracy: 0.9698\n",
      "Epoch 141/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0460 - accuracy: 0.9884 - val_loss: 0.0998 - val_accuracy: 0.9688\n",
      "Epoch 142/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0456 - accuracy: 0.9887 - val_loss: 0.1001 - val_accuracy: 0.9682\n",
      "Epoch 143/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0451 - accuracy: 0.9889 - val_loss: 0.0995 - val_accuracy: 0.9692\n",
      "Epoch 144/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0446 - accuracy: 0.9890 - val_loss: 0.0990 - val_accuracy: 0.9692\n",
      "Epoch 145/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0443 - accuracy: 0.9892 - val_loss: 0.0983 - val_accuracy: 0.9702\n",
      "Epoch 146/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0439 - accuracy: 0.9891 - val_loss: 0.0984 - val_accuracy: 0.9702\n",
      "Epoch 147/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0435 - accuracy: 0.9892 - val_loss: 0.0984 - val_accuracy: 0.9706\n",
      "Epoch 148/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0431 - accuracy: 0.9895 - val_loss: 0.0984 - val_accuracy: 0.9710\n",
      "Epoch 149/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0427 - accuracy: 0.9896 - val_loss: 0.0976 - val_accuracy: 0.9698\n",
      "Epoch 150/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0423 - accuracy: 0.9897 - val_loss: 0.0977 - val_accuracy: 0.9700\n",
      "Epoch 151/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0418 - accuracy: 0.9899 - val_loss: 0.0980 - val_accuracy: 0.9694\n",
      "Epoch 152/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0415 - accuracy: 0.9899 - val_loss: 0.0979 - val_accuracy: 0.9708\n",
      "Epoch 153/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0411 - accuracy: 0.9901 - val_loss: 0.0972 - val_accuracy: 0.9710\n",
      "Epoch 154/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0408 - accuracy: 0.9901 - val_loss: 0.0965 - val_accuracy: 0.9704\n",
      "Epoch 155/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0404 - accuracy: 0.9902 - val_loss: 0.0968 - val_accuracy: 0.9714\n",
      "Epoch 156/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0400 - accuracy: 0.9906 - val_loss: 0.0970 - val_accuracy: 0.9708\n",
      "Epoch 157/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0396 - accuracy: 0.9907 - val_loss: 0.0967 - val_accuracy: 0.9714\n",
      "Epoch 158/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0393 - accuracy: 0.9908 - val_loss: 0.0953 - val_accuracy: 0.9708\n",
      "Epoch 159/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0389 - accuracy: 0.9908 - val_loss: 0.0959 - val_accuracy: 0.9704\n",
      "Epoch 160/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0386 - accuracy: 0.9910 - val_loss: 0.0953 - val_accuracy: 0.9700\n",
      "Epoch 161/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0382 - accuracy: 0.9910 - val_loss: 0.0969 - val_accuracy: 0.9714\n",
      "Epoch 162/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0380 - accuracy: 0.9910 - val_loss: 0.0954 - val_accuracy: 0.9708\n",
      "Epoch 163/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0375 - accuracy: 0.9912 - val_loss: 0.0950 - val_accuracy: 0.9710\n",
      "Epoch 164/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0372 - accuracy: 0.9916 - val_loss: 0.0949 - val_accuracy: 0.9712\n",
      "Epoch 165/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0369 - accuracy: 0.9916 - val_loss: 0.0947 - val_accuracy: 0.9712\n",
      "Epoch 166/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0365 - accuracy: 0.9916 - val_loss: 0.0950 - val_accuracy: 0.9714\n",
      "Epoch 167/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0362 - accuracy: 0.9917 - val_loss: 0.0939 - val_accuracy: 0.9708\n",
      "Epoch 168/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0359 - accuracy: 0.9917 - val_loss: 0.0939 - val_accuracy: 0.9710\n",
      "Epoch 169/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0356 - accuracy: 0.9919 - val_loss: 0.0939 - val_accuracy: 0.9718\n",
      "Epoch 170/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0352 - accuracy: 0.9921 - val_loss: 0.0935 - val_accuracy: 0.9714\n",
      "Epoch 171/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0350 - accuracy: 0.9919 - val_loss: 0.0940 - val_accuracy: 0.9710\n",
      "Epoch 172/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0347 - accuracy: 0.9923 - val_loss: 0.0936 - val_accuracy: 0.9716\n",
      "Epoch 173/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0344 - accuracy: 0.9921 - val_loss: 0.0935 - val_accuracy: 0.9714\n",
      "Epoch 174/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0340 - accuracy: 0.9925 - val_loss: 0.0936 - val_accuracy: 0.9708\n",
      "Epoch 175/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0338 - accuracy: 0.9925 - val_loss: 0.0934 - val_accuracy: 0.9720\n",
      "Epoch 176/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0335 - accuracy: 0.9927 - val_loss: 0.0935 - val_accuracy: 0.9714\n",
      "Epoch 177/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0332 - accuracy: 0.9928 - val_loss: 0.0928 - val_accuracy: 0.9718\n",
      "Epoch 178/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0329 - accuracy: 0.9929 - val_loss: 0.0927 - val_accuracy: 0.9722\n",
      "Epoch 179/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0326 - accuracy: 0.9929 - val_loss: 0.0930 - val_accuracy: 0.9726\n",
      "Epoch 180/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0324 - accuracy: 0.9928 - val_loss: 0.0926 - val_accuracy: 0.9724\n",
      "Epoch 181/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0320 - accuracy: 0.9928 - val_loss: 0.0924 - val_accuracy: 0.9720\n",
      "Epoch 182/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0318 - accuracy: 0.9931 - val_loss: 0.0930 - val_accuracy: 0.9716\n",
      "Epoch 183/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0314 - accuracy: 0.9932 - val_loss: 0.0917 - val_accuracy: 0.9714\n",
      "Epoch 184/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0312 - accuracy: 0.9934 - val_loss: 0.0923 - val_accuracy: 0.9726\n",
      "Epoch 185/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0310 - accuracy: 0.9933 - val_loss: 0.0915 - val_accuracy: 0.9718\n",
      "Epoch 186/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0307 - accuracy: 0.9935 - val_loss: 0.0916 - val_accuracy: 0.9722\n",
      "Epoch 187/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0305 - accuracy: 0.9935 - val_loss: 0.0912 - val_accuracy: 0.9718\n",
      "Epoch 188/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0302 - accuracy: 0.9935 - val_loss: 0.0920 - val_accuracy: 0.9728\n",
      "Epoch 189/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0299 - accuracy: 0.9937 - val_loss: 0.0919 - val_accuracy: 0.9716\n",
      "Epoch 190/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0296 - accuracy: 0.9936 - val_loss: 0.0911 - val_accuracy: 0.9722\n",
      "Epoch 191/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0294 - accuracy: 0.9938 - val_loss: 0.0909 - val_accuracy: 0.9722\n",
      "Epoch 192/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0292 - accuracy: 0.9939 - val_loss: 0.0911 - val_accuracy: 0.9722\n",
      "Epoch 193/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0289 - accuracy: 0.9939 - val_loss: 0.0923 - val_accuracy: 0.9718\n",
      "Epoch 194/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0286 - accuracy: 0.9940 - val_loss: 0.0915 - val_accuracy: 0.9724\n",
      "Epoch 195/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0284 - accuracy: 0.9942 - val_loss: 0.0912 - val_accuracy: 0.9720\n",
      "Epoch 196/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0281 - accuracy: 0.9941 - val_loss: 0.0918 - val_accuracy: 0.9716\n",
      "Epoch 1/200\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.5305 - accuracy: 0.8610 - val_loss: 0.3243 - val_accuracy: 0.9092\n",
      "Epoch 2/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2580 - accuracy: 0.9264 - val_loss: 0.2425 - val_accuracy: 0.9310\n",
      "Epoch 3/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2044 - accuracy: 0.9415 - val_loss: 0.2038 - val_accuracy: 0.9410\n",
      "Epoch 4/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1688 - accuracy: 0.9516 - val_loss: 0.1889 - val_accuracy: 0.9442\n",
      "Epoch 5/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1438 - accuracy: 0.9592 - val_loss: 0.1627 - val_accuracy: 0.9528\n",
      "Epoch 6/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1252 - accuracy: 0.9648 - val_loss: 0.1555 - val_accuracy: 0.9542\n",
      "Epoch 7/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1102 - accuracy: 0.9682 - val_loss: 0.1398 - val_accuracy: 0.9586\n",
      "Epoch 8/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0981 - accuracy: 0.9727 - val_loss: 0.1317 - val_accuracy: 0.9600\n",
      "Epoch 9/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0885 - accuracy: 0.9757 - val_loss: 0.1215 - val_accuracy: 0.9618\n",
      "Epoch 10/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0796 - accuracy: 0.9777 - val_loss: 0.1170 - val_accuracy: 0.9634\n",
      "Epoch 11/200\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.0719 - accuracy: 0.9796 - val_loss: 0.1106 - val_accuracy: 0.9648\n",
      "Epoch 12/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0654 - accuracy: 0.9819 - val_loss: 0.1081 - val_accuracy: 0.9676\n",
      "Epoch 13/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.1029 - val_accuracy: 0.9674\n",
      "Epoch 14/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0545 - accuracy: 0.9848 - val_loss: 0.1010 - val_accuracy: 0.9664\n",
      "Epoch 15/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0497 - accuracy: 0.9864 - val_loss: 0.0998 - val_accuracy: 0.9690\n",
      "Epoch 16/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0458 - accuracy: 0.9879 - val_loss: 0.1004 - val_accuracy: 0.9688\n",
      "Epoch 17/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0424 - accuracy: 0.9889 - val_loss: 0.0924 - val_accuracy: 0.9728\n",
      "Epoch 18/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0386 - accuracy: 0.9903 - val_loss: 0.0934 - val_accuracy: 0.9706\n",
      "Epoch 19/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0355 - accuracy: 0.9913 - val_loss: 0.0889 - val_accuracy: 0.9730\n",
      "Epoch 20/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0328 - accuracy: 0.9922 - val_loss: 0.0886 - val_accuracy: 0.9738\n",
      "Epoch 21/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0302 - accuracy: 0.9928 - val_loss: 0.0971 - val_accuracy: 0.9694\n",
      "Epoch 22/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0280 - accuracy: 0.9938 - val_loss: 0.0884 - val_accuracy: 0.9722\n",
      "Epoch 23/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0257 - accuracy: 0.9941 - val_loss: 0.0904 - val_accuracy: 0.9712\n",
      "Epoch 24/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0240 - accuracy: 0.9951 - val_loss: 0.0867 - val_accuracy: 0.9744\n",
      "Epoch 25/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0220 - accuracy: 0.9958 - val_loss: 0.0888 - val_accuracy: 0.9736\n",
      "Epoch 26/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0203 - accuracy: 0.9962 - val_loss: 0.0857 - val_accuracy: 0.9740\n",
      "Epoch 27/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0190 - accuracy: 0.9968 - val_loss: 0.0852 - val_accuracy: 0.9756\n",
      "Epoch 28/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0176 - accuracy: 0.9973 - val_loss: 0.0853 - val_accuracy: 0.9750\n",
      "Epoch 29/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0162 - accuracy: 0.9977 - val_loss: 0.0876 - val_accuracy: 0.9740\n",
      "Epoch 30/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0152 - accuracy: 0.9980 - val_loss: 0.0863 - val_accuracy: 0.9742\n",
      "Epoch 31/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0141 - accuracy: 0.9981 - val_loss: 0.0875 - val_accuracy: 0.9742\n",
      "Epoch 32/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0131 - accuracy: 0.9985 - val_loss: 0.0869 - val_accuracy: 0.9746\n",
      "Epoch 1/200\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.4961 - accuracy: 0.8464 - val_loss: 0.1672 - val_accuracy: 0.9502\n",
      "Epoch 2/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1085 - accuracy: 0.9668 - val_loss: 0.1505 - val_accuracy: 0.9500\n",
      "Epoch 3/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0698 - accuracy: 0.9781 - val_loss: 1.0558 - val_accuracy: 0.8382\n",
      "Epoch 4/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0783 - accuracy: 0.9773 - val_loss: 0.0905 - val_accuracy: 0.9708\n",
      "Epoch 5/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0384 - accuracy: 0.9879 - val_loss: 0.0810 - val_accuracy: 0.9772\n",
      "Epoch 6/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0266 - accuracy: 0.9916 - val_loss: 0.0933 - val_accuracy: 0.9742\n",
      "Epoch 7/200\n",
      "215/215 [==============================] - 1s 7ms/step - loss: 0.0184 - accuracy: 0.9945 - val_loss: 0.0801 - val_accuracy: 0.9790\n",
      "Epoch 8/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0133 - accuracy: 0.9965 - val_loss: 0.0791 - val_accuracy: 0.9788\n",
      "Epoch 9/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0833 - val_accuracy: 0.9806\n",
      "Epoch 10/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.0876 - val_accuracy: 0.9792\n",
      "Epoch 11/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.0946 - val_accuracy: 0.9772\n",
      "Epoch 12/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.0915 - val_accuracy: 0.9804\n",
      "Epoch 13/200\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9786\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     11\u001b[0m mnist_mlp\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     13\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmnist_mlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmnist_mlp_early_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m model_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     20\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(model_loss)\n",
      "File \u001b[1;32m~\\Learning\\Data Science\\TensorFlow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Learning\\Data Science\\TensorFlow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "learning_rates = np.logspace(-4,2,7)\n",
    "losses = []\n",
    "mnist_mlp_weights = mnist_mlp.get_weights()\n",
    "mnist_mlp_early_stop = keras.callbacks.EarlyStopping(patience=5,restore_best_weights=True)\n",
    "model_min_loss = None\n",
    "min_loss = 99999\n",
    "for lr in learning_rates:\n",
    "    tf.keras.backend.clear_session()\n",
    "    mnist_mlp.set_weights(mnist_mlp_weights)\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "    mnist_mlp.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = mnist_mlp.fit(X_train,y_train,batch_size=256,epochs=200,\n",
    "                            callbacks=[mnist_mlp_early_stop],\n",
    "                            validation_data=(X_valid,y_valid))\n",
    "    model_loss = np.min(history.history['val_loss'])\n",
    "    losses.append(model_loss)\n",
    "    if model_loss < min_loss:\n",
    "        min_loss = model_loss\n",
    "        mnist_mlp.save(\"System\\\\model_min_loss.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e681c3-b2ee-42ca-a217-0aaf3f8a7893",
   "metadata": {},
   "source": [
    "Unfortunately, we have trained our model until our GPU is out of memory.\\\n",
    "But luckily, with just the portion that we succesfully run, we can somehow identify the optimum learning rate already.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d618c61-f8cc-4c98-b8bf-fefe651f0bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning_rates[:5],losses)\n",
    "plt.title(\"Losses vs Learning Rate\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a785c5-13a2-432a-8b46-08fc587a3803",
   "metadata": {},
   "source": [
    "We can see that the elbow exists at learning rate = 0.01.\\\n",
    "We will use learning rate = 1.0 as the best learning rate in this case, since it still provides lower loss compared to 0.01.\n",
    "\n",
    "We will also reload the optimal model with minimum loss that we saved earlier during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44ac1b-e0e7-4ac9-9324-189403508418",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede9a5e-9f16-4754-b77c-6f1f418ecb68",
   "metadata": {},
   "source": [
    "The best model's illustration is shown in the image below:\n",
    "\n",
    "<img src=\"Images/MNIST_Digits_MLP_Architecture_best.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb677738-b4ec-46d7-8c11-d8a0c96da89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_mlp_best = keras.models.load_model(\"System\\\\model_min_loss.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d39a32-61e9-463c-aec5-289b6ac8d75a",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddf009be-e47e-4b83-aff1-b5d333a330ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 5ms/step - loss: 0.0613 - accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.061335302889347076, 0.9815000295639038]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_mlp_best_acc = mnist_mlp_best.evaluate(X_test,y_test,batch_size=64)\n",
    "mnist_mlp_best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bcd8f6-d305-4aea-a8bc-a33db1b4f79e",
   "metadata": {},
   "source": [
    "We can achieve accuracy as high as 98% without even further fine tune the number of layers and number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6dae17-a904-4896-a061-0c808d575b5f",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "548dc1b9-ce9c-4b81-9a8c-245ef52b18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_mlp_best_scores = {\n",
    "    \"mnist_mlp_best_acc\":mnist_mlp_best_acc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcc82687-985d-4971-a015-22fce6c62e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Trained Models\\2022-08-29_00-14-04_mnist_mlp_best\\assets\n"
     ]
    }
   ],
   "source": [
    "dump_keras_model(mnist_mlp_best,filename=\"mnist_mlp_best\",scores=mnist_mlp_best_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87239ce6-96e5-42bb-b424-99e9e06a39ae",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a02552c-4559-4c67-aed4-9253c5d0b9a5",
   "metadata": {},
   "source": [
    "In addressing the memory error that I encountered during the iterative training phase to find the best learning rate, there are several ways that is potentially able to fix it.\n",
    "1. Simply using a stronger hardware specs, GPU with larger memory.\n",
    "2. Mini-batching the datasets into smaller batches before fitting into the model (due to tensorflow will store the whole dataset to the GPU memory regardless of batch_size in the .fit() method)\n",
    "3. Reduce the dimensionality of the datasets with technique such as PCA so we can greatly reduce the number of features in the dataset.\n",
    "\n",
    "But eitherway, with the best found learning rate, we are able to train a model that is very robust against test dataset, at 98% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
